{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c73770",
   "metadata": {},
   "source": [
    "# 5.1 - Evaluate finetuned models over each checkpoint (1 for epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e5226a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "from evaluation_function import evaluate_model\n",
    "from utils import define_model_name\n",
    "from store_load_results import store_results, load_results\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57cfdbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINDATA_FILE = os.path.join(os.getcwd(), '..', 'data', 'train_test_dataset', 'orange_qa_train.jsonl')\n",
    "TESTDATA_MCQ_FILE = os.path.join(os.getcwd(), '..', 'data', 'train_test_dataset', 'orange_qa_MCQ_test.jsonl')\n",
    "TESTDATA_MCQ_CON_FILE = os.path.join(os.getcwd(), '..', 'data', 'train_test_dataset', 'orange_qa_MCQ-con_test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c574b055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration: Qwen3-0.6B_DoRA_qvko_r8_alpha16_drop0.05_proj(qvko)_bs16_lr0.0001_ep20__\n"
     ]
    }
   ],
   "source": [
    "LORA_PROJECTIONS = \"qvko\"\n",
    "PROJECTIONS = {\n",
    "    \"q\": \"q_proj\",\n",
    "    \"k\": \"k_proj\",\n",
    "    \"v\": \"v_proj\",\n",
    "    \"o\": \"o_proj\",\n",
    "    \"g\": \"gate_proj\",\n",
    "    \"d\": \"down_proj\",\n",
    "    \"u\": \"up_proj\"\n",
    "}\n",
    "projections = [PROJECTIONS[p] for p in list(LORA_PROJECTIONS)]\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    \"base_model\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"finetuning\": True,\n",
    "    \"use_dora\": True,\n",
    "    \"n_epochs\": 20,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 16,\n",
    "    \"lora_projections\": projections,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"new_tokens_path\":  None,\n",
    "    \"new_tokens_init\": None,\n",
    "    \"new_tokens_train\": None,\n",
    "    \"wandb_project\": None,  # wandb project name\n",
    "}\n",
    "model_name, OUTPUT_DIR = define_model_name(MODEL_CONFIG)\n",
    "OUTPUT_DIR = OUTPUT_DIR.replace(\"notebooks/models\", \"notebooks/../models\")  # adjust for model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "973a7af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_both_datasets(model_config, output_dir, checkpoint_id:int=3640, epoch_id:int=20):\n",
    "    MODEL_CONFIG = model_config.copy()\n",
    "    OUTPUT_DIR = os.path.join(output_dir, f\"checkpoint-{checkpoint_id}\")\n",
    "    MODEL_CONFIG['n_epochs'] = epoch_id\n",
    "    MODEL_CONFIG['model_name'] = model_name.replace(\"ep20\", f\"ep20({epoch_id})\")\n",
    "\n",
    "    try:\n",
    "        results_dir = os.path.join(os.getcwd(), '..', 'results')\n",
    "        with open(os.path.join(results_dir, f'{MODEL_CONFIG[\"model_name\"]}_results.json'), 'r') as f:\n",
    "            return json.load(f)\n",
    "        print(\"Results already exist:\", results)\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(\"No existing results found. Proceeding with evaluation.\", str(e))\n",
    "\n",
    "    # 11. Load model and tokenizer\n",
    "    if MODEL_CONFIG['finetuning']:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "        # Load base model first, then load PEFT adapter\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_CONFIG['base_model'], \n",
    "            device_map=\"auto\", \n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        base_model.resize_token_embeddings(len(tokenizer))\n",
    "        model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_CONFIG['base_model'], device_map=\"auto\", torch_dtype=torch.float16)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['base_model'], trust_remote_code=True)\n",
    "\n",
    "    # 12. Evaluate and store results\n",
    "    with open(TESTDATA_MCQ_FILE, \"r\") as f:\n",
    "        test_mcq_dataset = json.load(f)\n",
    "    accuracy_mcq, se_mcq = evaluate_model(model, tokenizer, test_mcq_dataset, batch_size=MODEL_CONFIG['batch_size'])\n",
    "\n",
    "    with open(TESTDATA_MCQ_CON_FILE, \"r\") as f:\n",
    "        test_mcq_con_dataset = json.load(f)\n",
    "    accuracy_mcq_con, se_mcq_con = evaluate_model(model, tokenizer, test_mcq_con_dataset, batch_size=MODEL_CONFIG['batch_size'])\n",
    "    results = {\n",
    "        \"accuracy_mcq\": accuracy_mcq,\n",
    "        \"se_mcq\": se_mcq,\n",
    "        \"accuracy_mcq_con\": accuracy_mcq_con,\n",
    "        \"se_mcq_con\": se_mcq_con,\n",
    "    }\n",
    "    print(\"Evaluation Results:\", results)\n",
    "    results_dir = os.path.join(os.getcwd(), '..', 'results')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(results_dir, f'{MODEL_CONFIG[\"model_name\"]}_results.json'), 'w') as f:\n",
    "        json.dump(MODEL_CONFIG | results, f, indent=4)\n",
    "    print(\"Results stored successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fff78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating checkpoint 182 for epoch 1\n",
      "Evaluating checkpoint 364 for epoch 2\n",
      "Evaluating checkpoint 546 for epoch 3\n",
      "Evaluating checkpoint 728 for epoch 4\n",
      "Evaluating checkpoint 910 for epoch 5\n",
      "Evaluating checkpoint 1092 for epoch 6\n",
      "Evaluating checkpoint 1274 for epoch 7\n",
      "No existing results found. Proceeding with evaluation. [Errno 2] No such file or directory: '/root/dev/Tutorials/LoRA-finetuning-tutorial/notebooks/../results/Qwen3-0.6B_DoRA_qvko_r8_alpha16_drop0.05_proj(qvko)_bs16_lr0.0001_ep20(7)___results.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Evaluating model:   0%|          | 0/13 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating model:  38%|███▊      | 5/13 [01:40<04:17, 32.18s/it]"
     ]
    }
   ],
   "source": [
    "checkpoints = {\n",
    "    i: 182*i for i in range(1, 21)\n",
    "}\n",
    "for epoch, checkpoint in checkpoints.items():\n",
    "    print(f\"Evaluating checkpoint {checkpoint} for epoch {epoch}\")\n",
    "    evaluate_model_on_both_datasets(MODEL_CONFIG, OUTPUT_DIR, checkpoint_id=checkpoint, epoch_id=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82091bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
