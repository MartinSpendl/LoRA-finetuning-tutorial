{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9eea854",
   "metadata": {},
   "source": [
    "# 4.0 - Finetune Qwen3 0.6B on Orange QA train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcef8335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martin/Documents/FRI/Workshops/LoRA-tutorial/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MODEL_ID = \"Qwen/Qwen3-0.6B\"\n",
    "OUTPUT_DIR = os.path.join(os.getcwd(), '..', 'models', 'orange_qa_finetuned_Qwen3-0.6B_DoRA_qkvogdu')\n",
    "DATA_FILE = os.path.join(os.getcwd(), '..', 'data', 'train_test_dataset', 'orange_qa_train.jsonl')\n",
    "\n",
    "# 1. Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Fix: Qwen has no default pad token\n",
    "tokenizer_standard_tokens = len(tokenizer)\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=DATA_FILE, split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "863ab3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martin/Documents/FRI/Workshops/LoRA-tutorial/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=4,        # Rank (Higher = more parameters to train, smarter but slower)\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],# \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    use_dora=True, # <--- This enables DoRA (Better learning than standard LoRA)\n",
    ")\n",
    "\n",
    "model_dora = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f53e0a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[3379, 61706, 9086]], 'attention_mask': [[1, 1, 1]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['Select Rows widget'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "770c83f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens = ['Select Rows']\n",
    "tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eafe5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='Qwen/Qwen3-0.6B', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|im_end|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151669: AddedToken(\"Select Rows\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "953e59ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(151936, 1024), 151669)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dora.base_model.model.model.embed_tokens, tokenizer_standard_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef5a1cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0093,  0.0337, -0.0747,  ...,  0.0120, -0.0106,  0.0160],\n",
       "        [ 0.0320,  0.0238, -0.0593,  ..., -0.0023, -0.0349,  0.0090],\n",
       "        [ 0.0267,  0.0339, -0.0198,  ..., -0.0099,  0.0063,  0.0226],\n",
       "        ...,\n",
       "        [ 0.0060,  0.0131,  0.0190,  ...,  0.0020,  0.0075,  0.0057],\n",
       "        [ 0.0060,  0.0131,  0.0190,  ...,  0.0020,  0.0075,  0.0057],\n",
       "        [ 0.0060,  0.0131,  0.0190,  ...,  0.0020,  0.0075,  0.0057]],\n",
       "       device='mps:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dora.base_model.model.model.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be66526f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(151670, 1024)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e5e720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dora.base_model.model.model.embed_tokens = model_dora.base_model.model.model.embed_tokens.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "436403b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(151670, 1024), 151669)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dora.base_model.model.model.embed_tokens, tokenizer_standard_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fde4c976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0046, -0.0576,  0.0605,  ..., -0.0118, -0.0267,  0.0042],\n",
       "         [ 0.0276, -0.0500,  0.0479,  ..., -0.0098, -0.0055, -0.0187],\n",
       "         [ 0.0052, -0.0593,  0.0151,  ...,  0.0080,  0.0105,  0.0166],\n",
       "         [ 0.0063, -0.0505,  0.0297,  ..., -0.0197, -0.0007,  0.0067],\n",
       "         [ 0.0192, -0.0581,  0.0393,  ..., -0.0009, -0.0107,  0.0258]],\n",
       "        device='mps:0'),\n",
       " 151377)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dora.base_model.model.lm_head.weight[-293:-288], model_dora.base_model.model.lm_head.weight.shape[0] - 293"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1c96b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight: False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.0.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.0.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.0.input_layernorm.weight: False\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.1.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.1.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.1.input_layernorm.weight: False\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.2.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.2.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.2.input_layernorm.weight: False\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.3.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.3.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.3.input_layernorm.weight: False\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.4.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.4.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.4.input_layernorm.weight: False\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.5.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.5.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.5.input_layernorm.weight: False\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.6.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.6.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.6.input_layernorm.weight: False\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.7.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.7.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.7.input_layernorm.weight: False\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.8.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.8.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.8.input_layernorm.weight: False\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.9.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.9.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.9.input_layernorm.weight: False\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.10.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.10.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.10.input_layernorm.weight: False\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.11.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.11.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.11.input_layernorm.weight: False\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.12.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.12.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.12.input_layernorm.weight: False\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.13.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.13.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.13.input_layernorm.weight: False\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.14.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.14.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.14.input_layernorm.weight: False\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.15.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.15.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.15.input_layernorm.weight: False\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.16.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.16.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.16.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.16.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.16.input_layernorm.weight: False\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.17.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.17.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.17.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.17.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.17.input_layernorm.weight: False\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.18.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.18.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.18.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.18.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.18.input_layernorm.weight: False\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.19.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.19.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.19.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.19.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.19.input_layernorm.weight: False\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.20.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.20.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.20.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.20.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.20.input_layernorm.weight: False\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.21.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.21.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.21.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.21.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.21.input_layernorm.weight: False\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.22.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.22.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.22.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.22.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.22.input_layernorm.weight: False\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.23.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.23.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.23.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.23.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.23.input_layernorm.weight: False\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.24.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.24.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.24.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.24.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.24.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.24.input_layernorm.weight: False\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.25.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.25.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.25.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.25.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.25.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.25.input_layernorm.weight: False\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.26.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.26.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.26.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.26.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.26.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.26.input_layernorm.weight: False\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight: False\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight: False\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: True\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: True\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight: True\n",
      "base_model.model.model.layers.27.self_attn.q_norm.weight: False\n",
      "base_model.model.model.layers.27.self_attn.k_norm.weight: False\n",
      "base_model.model.model.layers.27.mlp.gate_proj.weight: False\n",
      "base_model.model.model.layers.27.mlp.up_proj.weight: False\n",
      "base_model.model.model.layers.27.mlp.down_proj.weight: False\n",
      "base_model.model.model.layers.27.input_layernorm.weight: False\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight: False\n",
      "base_model.model.model.norm.weight: False\n"
     ]
    }
   ],
   "source": [
    "for name, params in model_dora.named_parameters():\n",
    "    print(f\"{name}: {params.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "762426ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dora.base_model.model.lm_head.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f1d7ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x321010e90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def zero_out_old_token_grads(grad):\n",
    "    new_grad = grad.clone()\n",
    "    new_grad[:tokenizer_standard_tokens, :] = 0.0\n",
    "    return new_grad\n",
    "\n",
    "model_dora.base_model.model.model.embed_tokens.weight.requires_grad = True\n",
    "model_dora.base_model.model.model.embed_tokens.weight.register_hook(zero_out_old_token_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99e536ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0034, -0.1084,  0.0204,  ..., -0.0077,  0.0016, -0.0035],\n",
       "        [ 0.0050, -0.1035,  0.0209,  ...,  0.0030, -0.0085, -0.0027],\n",
       "        [ 0.0022, -0.1118,  0.0204,  ...,  0.0018, -0.0019, -0.0084],\n",
       "        [ 0.0013, -0.1099,  0.0286,  ..., -0.0013, -0.0171,  0.0019],\n",
       "        [ 0.0061, -0.0928,  0.0251,  ...,  0.0051, -0.0082, -0.0054]],\n",
       "       device='mps:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dora.base_model.model.model.embed_tokens.weight[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d94f632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0034, -0.1084,  0.0204,  ..., -0.0077,  0.0016, -0.0035],\n",
       "        [ 0.0050, -0.1035,  0.0209,  ...,  0.0030, -0.0085, -0.0027],\n",
       "        [ 0.0022, -0.1118,  0.0204,  ...,  0.0018, -0.0019, -0.0084],\n",
       "        [ 0.0013, -0.1099,  0.0286,  ..., -0.0013, -0.0171,  0.0019],\n",
       "        [-0.0043, -0.0822,  0.0172,  ..., -0.0091, -0.0062, -0.0012]],\n",
       "       device='mps:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dora.base_model.model.model.embed_tokens.weight[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44b77e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martin/Documents/FRI/Workshops/LoRA-tutorial/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='151' max='151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [151/151 03:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.880500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.857400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.950800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.985800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.802700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.888200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.782500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.899500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.877700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.839300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.745600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.827400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.894900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.858200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.738400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.758900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.795100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.725200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.766800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.796000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.655400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.793400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.784300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.649100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.767500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.687400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.682500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.686300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.723100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.693200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.819000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.639600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.726200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.702100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.831200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.778900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.599300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.849500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.612100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.742600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.738600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.695500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.799400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.743700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.705300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.602800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.698100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.734200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.628300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.682900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.727500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.676400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.734200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.701300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.669900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.650600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.671600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.679800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martin/Documents/FRI/Workshops/LoRA-tutorial/.venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to /Users/martin/Documents/FRI/Workshops/LoRA-tutorial/notebooks/../models/orange_qa_finetuned_Qwen3-0.6B_DoRA_qkvogdu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martin/Documents/FRI/Workshops/LoRA-tutorial/.venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 6. Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,          # How many times to read the docs\n",
    "    per_device_train_batch_size=4, \n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-3,\n",
    "    fp16=True,                   # Use mixed precision\n",
    "    logging_steps=2,\n",
    "    optim=\"adamw_torch\",   # Saves memory\n",
    "    save_strategy=\"epoch\",       # Save a checkpoint every epoch\n",
    ")\n",
    "\n",
    "# 7. Initialize Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model_dora,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# 8. Train & Save\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e803b23f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
