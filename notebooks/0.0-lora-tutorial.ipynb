{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd7c256",
   "metadata": {},
   "source": [
    "# Hands-on LoRA finetuning tutorial on Orange3 QA & MCQ dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a32a07",
   "metadata": {},
   "source": [
    "### Preparing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22242794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ðŸ› ï¸ Setup: Clone Repo & Install Dependencies\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Define your repository\n",
    "REPO_URL = \"https://github.com/MartinSpendl/LoRA-finetuning-tutorial.git\"\n",
    "REPO_NAME = \"LoRA-finetuning-tutorial\"\n",
    "branch = \"main\"\n",
    "\n",
    "# 2. Clone or Pull (if it already exists)\n",
    "if os.path.isdir(REPO_NAME):\n",
    "    print(f\"ðŸ”„ Updating {REPO_NAME}...\")\n",
    "    !cd {REPO_NAME} && git pull origin {branch}\n",
    "else:\n",
    "    print(f\"ðŸ“¥ Cloning {REPO_NAME}...\")\n",
    "    !git clone {REPO_URL}\n",
    "\n",
    "# 3. Add the repo to Python's path so you can import from it\n",
    "if REPO_NAME not in sys.path:\n",
    "    sys.path.append(os.path.abspath(REPO_NAME))\n",
    "print(f\"âœ… Added '{REPO_NAME}' to Python path.\")\n",
    "\n",
    "# 4. Install dependencies from your repo's requirements.txt\n",
    "print(\"ðŸ“¦ Installing dependencies...\")\n",
    "!pip install -q -r {REPO_NAME}/requirements.txt\n",
    "\n",
    "print(\"ðŸš€ Ready to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a95f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import sys\n",
    "sys.path.append('LoRA-finetuning-tutorial/src')\n",
    "sys.path.append('../src')\n",
    "from store_load_results import store_results, load_results\n",
    "from evaluation_function import evaluate_model\n",
    "from utils import define_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19405c6b",
   "metadata": {},
   "source": [
    "## PART 1: Basic Finetuning with LoRA using PEFT\n",
    "\n",
    "### Defining the model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    \"base_model\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"finetuning\": True,\n",
    "    \"use_dora\": True,\n",
    "    \"n_epochs\": 1,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lr\": 5e-4,\n",
    "    \"batch_size\": 8,\n",
    "    \"lora_projections\": \"qvko\",\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"new_tokens_path\": None,\n",
    "    \"new_tokens_init\": \"random\",\n",
    "    \"new_tokens_train\": True,\n",
    "}\n",
    "\n",
    "PROJECTIONS = {\n",
    "    \"q\": \"q_proj\",\n",
    "    \"k\": \"k_proj\",\n",
    "    \"v\": \"v_proj\",\n",
    "    \"o\": \"o_proj\",\n",
    "    \"g\": \"gate_proj\",\n",
    "    \"d\": \"down_proj\",\n",
    "    \"u\": \"up_proj\"\n",
    "}\n",
    "\n",
    "projections = [PROJECTIONS[p] for p in list(MODEL_CONFIG[\"lora_projections\"])]\n",
    "MODEL_CONFIG['lora_projections'] = projections\n",
    "model_name, OUTPUT_DIR = define_model_name(MODEL_CONFIG)\n",
    "\n",
    "wandb_project = \"qwen3-lora-finetuning\"\n",
    "wandb_run_name = model_name\n",
    "os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "\n",
    "print(\"Model configuration:\")\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7799f7a",
   "metadata": {},
   "source": [
    "## Training dataset\n",
    "\n",
    "Let's look at the training dataset, and what kind of data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c8e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINDATA_FILE = os.path.join(os.getcwd(), 'LoRA-finetuning-tutorial', 'data', 'train_test_dataset', 'orange_qa_train.jsonl')\n",
    "TESTDATA_MCQ_FILE = os.path.join(os.getcwd(), 'LoRA-finetuning-tutorial', 'data', 'train_test_dataset', 'orange_qa_MCQ_test.jsonl')\n",
    "TESTDATA_MCQ_CON_FILE = os.path.join(os.getcwd(), 'LoRA-finetuning-tutorial', 'data', 'train_test_dataset', 'orange_qa_MCQ-con_test.jsonl')\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=TRAINDATA_FILE, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b21b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check different question types:\n",
    "# QA: 7\n",
    "# MCQ: 579\n",
    "# QA - connection: 0\n",
    "# MCQ - conection: 1012\n",
    "\n",
    "ID = 0\n",
    "sample = dataset[ID]['messages']\n",
    "for message in sample:\n",
    "    print(\"Role:\", message['role'])\n",
    "    print(message['content'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d98174",
   "metadata": {},
   "source": [
    "### Load the model and tokenizer\n",
    "\n",
    "Let's get familiar with the model and tokenizer. Specifically look at what layers the model has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f54aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load Model & Tokenizer\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['base_model'])\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_CONFIG['base_model'],\n",
    "    dtype=torch.float16,       # Use float16 to save memory\n",
    "    device_map=\"auto\",          # Auto-selects GPU or CPU\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2998bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Orange data mining\", \"widget\", \"Hierarchical Clustering\", \"-->\", \"-x->\"\n",
    "tokenizer([\"Orange Data Mining\", \"orange data mining\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26506a",
   "metadata": {},
   "source": [
    "### PEFT Module for LoRA/DoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # from config\n",
    "    target_modules=MODEL_CONFIG['lora_projections'],\n",
    "    use_dora=MODEL_CONFIG['use_dora'],\n",
    "    r=MODEL_CONFIG['lora_r'],\n",
    "    lora_alpha=MODEL_CONFIG['lora_alpha'],\n",
    "    lora_dropout=MODEL_CONFIG['lora_dropout'],\n",
    "    modules_to_save=None,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3019c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161045fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.base_model.model.model.layers[0].self_attn.named_parameters():\n",
    "    print(name, param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64671df3",
   "metadata": {},
   "source": [
    "## Setup training arguments and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd7e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=MODEL_CONFIG['n_epochs'],          # How many times to read the docs\n",
    "    per_device_train_batch_size=MODEL_CONFIG['batch_size'], \n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=MODEL_CONFIG['lr'],\n",
    "    fp16=True,                   # Use mixed precision\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_torch\",   \n",
    "    save_strategy=\"epoch\",       # Save a checkpoint every epoch\n",
    "    report_to=[\"wandb\"],         # Enable wandb logging\n",
    "    run_name=wandb_run_name,     # Set run name to model name\n",
    ")\n",
    "\n",
    "# 8. Initialize Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec7cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f4bf0",
   "metadata": {},
   "source": [
    "### Evaluate the model on MCQ and MCQ-connection test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Evaluate and store results\n",
    "with open(TESTDATA_MCQ_FILE, \"r\") as f:\n",
    "    test_mcq_dataset = json.load(f)\n",
    "accuracy_mcq, se_mcq = evaluate_model(model, tokenizer, test_mcq_dataset, batch_size=MODEL_CONFIG['batch_size'])\n",
    "\n",
    "with open(TESTDATA_MCQ_CON_FILE, \"r\") as f:\n",
    "    test_mcq_con_dataset = json.load(f)\n",
    "accuracy_mcq_con, se_mcq_con = evaluate_model(model, tokenizer, test_mcq_con_dataset, batch_size=MODEL_CONFIG['batch_size'])\n",
    "results = {\n",
    "    \"accuracy_mcq\": accuracy_mcq,\n",
    "    \"se_mcq\": se_mcq,\n",
    "    \"accuracy_mcq_con\": accuracy_mcq_con,\n",
    "    \"se_mcq_con\": se_mcq_con,\n",
    "}\n",
    "print(\"Evaluation Results:\", results)\n",
    "store_results(results, MODEL_CONFIG)\n",
    "print(\"Results stored successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0add05",
   "metadata": {},
   "source": [
    "## PART 2: Token-injection of Orange3 widgets\n",
    "\n",
    "If you are running out of GPU memory, you can restart the session and run the following cell again.\n",
    "\n",
    "Just make sure to also run cells from here onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09220ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete if still in memory to free up space\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae3e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import sys\n",
    "sys.path.append('LoRA-finetuning-tutorial/src')\n",
    "sys.path.append('../src')\n",
    "from store_load_results import store_results, load_results\n",
    "from evaluation_function import evaluate_model\n",
    "from utils import define_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {\n",
    "    \"base_model\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"finetuning\": True,\n",
    "    \"use_dora\": True,\n",
    "    \"n_epochs\": 1,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lr\": 5e-4,\n",
    "    \"batch_size\": 8,\n",
    "    \"lora_projections\": \"qvko\",\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"new_tokens_path\": \"LoRA-finetuning-tutorial/data/injected_tokens.json\", ## path to new tokens file\n",
    "    \"new_tokens_init\": \"random\",\n",
    "    \"new_tokens_train\": True,\n",
    "}\n",
    "\n",
    "PROJECTIONS = {\n",
    "    \"q\": \"q_proj\",\n",
    "    \"k\": \"k_proj\",\n",
    "    \"v\": \"v_proj\",\n",
    "    \"o\": \"o_proj\",\n",
    "    \"g\": \"gate_proj\",\n",
    "    \"d\": \"down_proj\",\n",
    "    \"u\": \"up_proj\"\n",
    "}\n",
    "\n",
    "projections = [PROJECTIONS[p] for p in list(MODEL_CONFIG[\"lora_projections\"])]\n",
    "MODEL_CONFIG['lora_projections'] = projections\n",
    "model_name, OUTPUT_DIR = define_model_name(MODEL_CONFIG)\n",
    "\n",
    "wandb_project = \"qwen3-lora-finetuning\"\n",
    "wandb_run_name = model_name\n",
    "os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "\n",
    "print(\"Model configuration:\")\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dab907",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINDATA_FILE = os.path.join(os.getcwd(), 'LoRA-finetuning-tutorial', 'data', 'train_test_dataset', 'orange_qa_train.jsonl')\n",
    "TESTDATA_MCQ_FILE = os.path.join(os.getcwd(), 'LoRA-finetuning-tutorial', 'data', 'train_test_dataset', 'orange_qa_MCQ_test.jsonl')\n",
    "TESTDATA_MCQ_CON_FILE = os.path.join(os.getcwd(), 'LoRA-finetuning-tutorial', 'data', 'train_test_dataset', 'orange_qa_MCQ-con_test.jsonl')\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=TRAINDATA_FILE, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb93bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['base_model'])\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_CONFIG['base_model'],\n",
    "    dtype=torch.float16,       # Use float16 to save memory\n",
    "    device_map=\"auto\",          # Auto-selects GPU or CPU\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4895a5",
   "metadata": {},
   "source": [
    "### Lets check the tokenizer again, and how it handles some common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eaaf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fcbad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer([\"Hierarchical Clustering\", \"-->\", \"-x->\", \"Logistic Regression\", \"Hierarchical Clustering -x-> Logistic Regression\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b936c5b2",
   "metadata": {},
   "source": [
    "### Adding new tokens to tokenizer & Extending model embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AddedToken\n",
    "\n",
    "# 5. Extend token library (tokenizer + model)\n",
    "if MODEL_CONFIG['new_tokens_path'] is not None:\n",
    "    print(\"Adding new tokens from:\", MODEL_CONFIG['new_tokens_path'])\n",
    "    DEFAULT_TOKEN_NUM = len(tokenizer)\n",
    "\n",
    "    with open(MODEL_CONFIG['new_tokens_path'], 'r') as f:\n",
    "        new_tokens_text = json.load(f)\n",
    "    new_token_tokenized = [tokenizer.encode(token) for token in new_tokens_text]\n",
    "    new_tokens = [\n",
    "        AddedToken(token, lstrip=True, rstrip=True)\n",
    "        for token in new_tokens_text\n",
    "    ]\n",
    "\n",
    "    ## extend tokenizer and model\n",
    "    print(\"Number of default tokenizer tokens:\", DEFAULT_TOKEN_NUM)\n",
    "    tokenizer.add_tokens(new_tokens)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(\"Number of new tokenizer tokens:\", len(tokenizer))\n",
    "\n",
    "    new_tokens_ids = [\n",
    "        tokenizer.encode(token)[0]\n",
    "        for token in new_tokens_text\n",
    "    ]\n",
    "    old_tokens_ids = [id for id in range(DEFAULT_TOKEN_NUM) if id not in new_tokens_ids]\n",
    "\n",
    "    ## initialize new token embeddings\n",
    "    model.model.embed_tokens = model.model.embed_tokens.float()\n",
    "\n",
    "    if MODEL_CONFIG['new_tokens_init'] == \"average\":\n",
    "        with torch.no_grad():\n",
    "            existing_embeddings = model.model.embed_tokens.weight\n",
    "            for new_id, previous_token_ids in zip(new_tokens_ids, new_token_tokenized):\n",
    "                model.model.embed_tokens.weight[new_id, :] = existing_embeddings[previous_token_ids].mean(dim=0)\n",
    "    elif MODEL_CONFIG['new_tokens_init'] == \"zero\":\n",
    "        with torch.no_grad():\n",
    "            model.model.embed_tokens[DEFAULT_TOKEN_NUM:, :].fill_(0.0)\n",
    "    elif MODEL_CONFIG['new_tokens_init'] == \"random\":\n",
    "        pass  # already quazi randomly initialized\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown new_tokens_init method: {MODEL_CONFIG['new_tokens_init']}\")\n",
    "\n",
    "    ## setup trainable weights for new tokens\n",
    "    if MODEL_CONFIG['new_tokens_train']:\n",
    "        def zero_out_old_token_grads(grad):\n",
    "            new_grad = grad.clone()\n",
    "            new_grad[old_tokens_ids, :] = 0.0\n",
    "            return new_grad\n",
    "        model.model.embed_tokens.weight.requires_grad = True\n",
    "        model.model.embed_tokens.weight.register_hook(zero_out_old_token_grads)\n",
    "    model.lm_head.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44706a9a",
   "metadata": {},
   "source": [
    "### Let's test our tokenizer with new tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c065ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38acdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer([\"Hierarchical Clustering\", \"-->\", \"-x->\", \"Logistic Regression\", \"Hierarchical Clustering -x-> Logistic Regression\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_CONFIG['new_tokens_path'] is not None:\n",
    "    modules_to_save = [\"lm_head\"] + ([\"embed_tokens\"] if MODEL_CONFIG['new_tokens_train'] else [])\n",
    "else:\n",
    "    modules_to_save = None\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # from config\n",
    "    target_modules=MODEL_CONFIG['lora_projections'],\n",
    "    use_dora=MODEL_CONFIG['use_dora'],\n",
    "    r=MODEL_CONFIG['lora_r'],\n",
    "    lora_alpha=MODEL_CONFIG['lora_alpha'],\n",
    "    lora_dropout=MODEL_CONFIG['lora_dropout'],\n",
    "    modules_to_save=modules_to_save,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d89cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LM head module requires grad: \", model.base_model.model.lm_head.modules_to_save.default.weight.requires_grad)\n",
    "print(\"Embed tokens module requires grad: \", model.base_model.model.model.embed_tokens.modules_to_save.default.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9563256",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_project = MODEL_CONFIG.get('wandb_project') or os.environ.get(\"WANDB_PROJECT\", \"qwen3-lora-finetuning\")\n",
    "wandb_run_name = MODEL_CONFIG['model_name']\n",
    "\n",
    "# Set wandb project via environment variable\n",
    "os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=MODEL_CONFIG['n_epochs'],          # How many times to read the docs\n",
    "    per_device_train_batch_size=MODEL_CONFIG['batch_size'], \n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=MODEL_CONFIG['lr'],\n",
    "    fp16=True,                   # Use mixed precision\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_torch\",   \n",
    "    save_strategy=\"epoch\",       # Save a checkpoint every epoch\n",
    "    report_to=[\"wandb\"],         # Enable wandb logging\n",
    "    run_name=wandb_run_name,     # Set run name to model name\n",
    ")\n",
    "\n",
    "# 8. Initialize Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf0cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training in FP32, now we have to cast back to FP16 to evaluate\n",
    "model.base_model.model.model.embed_tokens.modules_to_save.default = model.base_model.model.model.embed_tokens.modules_to_save.default.half()\n",
    "model.base_model.model.lm_head.modules_to_save.default = model.base_model.model.lm_head.modules_to_save.default.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Evaluate and store results\n",
    "with open(TESTDATA_MCQ_FILE, \"r\") as f:\n",
    "    test_mcq_dataset = json.load(f)\n",
    "accuracy_mcq, se_mcq = evaluate_model(model, tokenizer, test_mcq_dataset, batch_size=MODEL_CONFIG['batch_size'])\n",
    "\n",
    "with open(TESTDATA_MCQ_CON_FILE, \"r\") as f:\n",
    "    test_mcq_con_dataset = json.load(f)\n",
    "accuracy_mcq_con, se_mcq_con = evaluate_model(model, tokenizer, test_mcq_con_dataset, batch_size=MODEL_CONFIG['batch_size'])\n",
    "results = {\n",
    "    \"accuracy_mcq\": accuracy_mcq,\n",
    "    \"se_mcq\": se_mcq,\n",
    "    \"accuracy_mcq_con\": accuracy_mcq_con,\n",
    "    \"se_mcq_con\": se_mcq_con,\n",
    "}\n",
    "print(\"\\nEvaluation Results:\", results)\n",
    "store_results(results, MODEL_CONFIG)\n",
    "print(\"Results stored successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
